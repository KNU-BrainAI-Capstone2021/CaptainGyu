{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colaboratory에 오신 것을 환영합니다",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alscjf909/torch_GAN/blob/main/DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjagYUFw0J4i",
        "outputId": "e73998df-44c1-48de-e276-5a9c687a9e98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torch==1.4.0 torchvision==0.5.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/3b/fa92ece1e58a6a48ec598bab327f39d69808133e5b2fb33002ca754e381e/torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4MB 18kB/s \n",
            "\u001b[?25hCollecting torchvision==0.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/32/cb0e4c43cd717da50258887b088471568990b5a749784c465a8a1962e021/torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0MB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "  Found existing installation: torchvision 0.9.0+cu101\n",
            "    Uninstalling torchvision-0.9.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.9.0+cu101\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtg531XM0MMy",
        "outputId": "b2f8a131-6c33-40a2-d1e5-4d5427548caa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(torch.cuda.get_device_name())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Ck5ep10S_u"
      },
      "source": [
        "!mkdir input\n",
        "!mkdir outputs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82GV8FV30W1D",
        "outputId": "4d273f48-43ff-45e7-c251-fa3a1080a3e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile dcgan.py\n",
        "import torch.nn as nn\n",
        "\n",
        "# generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, nz):\n",
        "        super(Generator, self).__init__()\n",
        "        self.nz = nz\n",
        "        self.main = nn.Sequential(\n",
        "            # nz will be the input to the first convolution\n",
        "            nn.ConvTranspose2d(\n",
        "                nz, 512, kernel_size=4, \n",
        "                stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(\n",
        "                512, 256, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(\n",
        "                256, 128, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(\n",
        "                128, 64, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.ConvTranspose2d(\n",
        "                64, 3, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "\n",
        "# discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                3, 64, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                64, 128, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                128, 256, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                256, 512, kernel_size=4, \n",
        "                stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(\n",
        "                512, 1, kernel_size=4, \n",
        "                stride=1, padding=0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing dcgan.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTvCKvot0XWu",
        "outputId": "10d66db4-8412-4de6-8c31-23c4dcfdb6b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile utils.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# set the computation device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def label_real(size):\n",
        "    \"\"\"\n",
        "    Fucntion to create real labels (ones)\n",
        "    :param size: batch size\n",
        "    :return real label vector\n",
        "    \"\"\"\n",
        "    data = torch.ones(size, 1)\n",
        "    return data.to(device)\n",
        "\n",
        "def label_fake(size):\n",
        "    \"\"\"\n",
        "    Fucntion to create fake labels (zeros)\n",
        "    :param size: batch size\n",
        "    :returns fake label vector\n",
        "    \"\"\"\n",
        "    data = torch.zeros(size, 1)\n",
        "    return data.to(device)\n",
        "\n",
        "def create_noise(sample_size, nz):\n",
        "    \"\"\"\n",
        "    Fucntion to create noise\n",
        "    :param sample_size: fixed sample size or batch size\n",
        "    :param nz: latent vector size\n",
        "    :returns random noise vector\n",
        "    \"\"\"\n",
        "    return torch.randn(sample_size, nz, 1, 1).to(device)\n",
        "\n",
        "def save_generator_image(image, path):\n",
        "    \"\"\"\n",
        "    Function to save torch image batches\n",
        "    :param image: image tensor batch\n",
        "    :param path: path name to save image\n",
        "    \"\"\"\n",
        "    save_image(image, path, normalize=True)\n",
        "\n",
        "def weights_init(m):\n",
        "    \"\"\"\n",
        "    This function initializes the model weights randomly from a \n",
        "    Normal distribution. This follows the specification from the DCGAN paper.\n",
        "    https://arxiv.org/pdf/1511.06434.pdf\n",
        "    Source: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hPoJbzB0Zb-",
        "outputId": "cfc0b103-fe8a-4214-9592-5fb991a57a17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile train_dcgan.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as datasets\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "\n",
        "from utils import save_generator_image, weights_init\n",
        "from utils import label_fake, label_real, create_noise\n",
        "from dcgan import Generator, Discriminator\n",
        "from torch.utils.data import DataLoader\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "# learning parameters / configurations according to paper\n",
        "image_size = 64 # we need to resize image to 64x64\n",
        "batch_size = 128\n",
        "nz = 100 # latent vector size\n",
        "beta1 = 0.5 # beta1 value for Adam optimizer\n",
        "lr = 0.0002 # learning rate according to paper\n",
        "sample_size = 64 # fixed sample size\n",
        "epochs = 25 # number of epoch to train\n",
        "\n",
        "# set the computation device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# image transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), \n",
        "    (0.5, 0.5, 0.5)),\n",
        "])\n",
        "to_pil_image = transforms.ToPILImage() # gif를 생성하고 싶을때 사용 -> epochs 동안 생성 되는 이미지들을 gif로 변환시킴\n",
        "\n",
        "# prepare the data\n",
        "train_data = datasets.CIFAR10(\n",
        "    root='input/data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# initialize models\n",
        "generator = Generator(nz).to(device)\n",
        "discriminator = Discriminator().to(device)\n",
        "\n",
        "# initialize generator weights\n",
        "generator.apply(weights_init)\n",
        "# initialize discriminator weights\n",
        "discriminator.apply(weights_init)\n",
        "\n",
        "print('##### GENERATOR #####')\n",
        "print(generator)\n",
        "print('######################')\n",
        "\n",
        "print('\\n##### DISCRIMINATOR #####')\n",
        "print(discriminator)\n",
        "print('######################')\n",
        "\n",
        "# optimizers\n",
        "optim_g = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optim_d = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "\n",
        "# loss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "losses_g = [] # to store generator loss after each epoch\n",
        "losses_d = [] # to store discriminator loss after each epoch\n",
        "images = []\n",
        "\n",
        "# function to train the discriminator network\n",
        "def train_discriminator(optimizer, data_real, data_fake):\n",
        "    b_size = data_real.size(0)\n",
        "    # get the real label vector\n",
        "    real_label = label_real(b_size)\n",
        "    # get the fake label vector\n",
        "    fake_label = label_fake(b_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # get the outputs by doing real data forward pass\n",
        "    output_real = discriminator(data_real).view(-1)\n",
        "    loss_real = criterion(output_real, real_label)\n",
        "\n",
        "    # get the outputs by doing fake data forward pass\n",
        "    output_fake = discriminator(data_fake)\n",
        "    loss_fake = criterion(output_fake, fake_label)\n",
        "\n",
        "    # compute gradients of real loss \n",
        "    loss_real.backward()\n",
        "    # compute gradients of fake loss\n",
        "    loss_fake.backward()\n",
        "    # update discriminator parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss_real + loss_fake\n",
        "\n",
        "# function to train the generator network\n",
        "def train_generator(optimizer, data_fake):\n",
        "    b_size = data_fake.size(0)\n",
        "    # get the real label vector\n",
        "    real_label = label_real(b_size)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # output by doing a forward pass of the fake data through discriminator\n",
        "    output = discriminator(data_fake)\n",
        "    loss = criterion(output, real_label)\n",
        "\n",
        "    # compute gradients of loss\n",
        "    loss.backward()\n",
        "    # update generator parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss    \n",
        "\n",
        "# create the noise vector\n",
        "noise = create_noise(sample_size, nz)\n",
        "# print('SIZE', noise.size())\n",
        "# print('NOISE', noise)\n",
        "\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    loss_g = 0.0\n",
        "    loss_d = 0.0\n",
        "    for bi, data in tqdm(enumerate(train_loader), total=int(len(train_data)/train_loader.batch_size)):\n",
        "        image, _ = data\n",
        "        image = image.to(device)\n",
        "        b_size = len(image)\n",
        "        # forward pass through generator to create fake data\n",
        "        data_fake = generator(create_noise(b_size, nz)).detach()\n",
        "        data_real = image\n",
        "        loss_d += train_discriminator(optim_d, data_real, data_fake)\n",
        "        data_fake = generator(create_noise(b_size, nz))\n",
        "        loss_g += train_generator(optim_g, data_fake)\n",
        "\n",
        "    # final forward pass through generator to create fake data...\n",
        "    # ...after training for current epoch\n",
        "    generated_img = generator(noise).cpu().detach()\n",
        "    # save the generated torch tensor models to disk\n",
        "    save_generator_image(generated_img, f\"outputs/gen_img{epoch}.png\")\n",
        "    epoch_loss_g = loss_g / bi # total generator loss for the epoch\n",
        "    epoch_loss_d = loss_d / bi # total discriminator loss for the epoch\n",
        "    losses_g.append(epoch_loss_g)\n",
        "    losses_d.append(epoch_loss_d)\n",
        "    images.append(generated_img)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} of {epochs}\")\n",
        "    print(f\"Generator loss: {epoch_loss_g:.8f}, Discriminator loss: {epoch_loss_d:.8f}\")\n",
        "\n",
        "print('DONE TRAINING')\n",
        "# save the model weights to disk\n",
        "torch.save(generator.state_dict(), 'outputs/generator.pth')\n",
        "\n",
        "# save the generated images as GIF file\n",
        "imgs = [np.array(to_pil_image(img)) for img in images]\n",
        "imageio.mimsave('../outputs/generator_images.gif', imgs)\n",
        "\n",
        "# plot and save the generator and discriminator loss\n",
        "plt.figure()\n",
        "plt.plot(losses_g, label='Generator loss')\n",
        "plt.plot(losses_d, label='Discriminator Loss')\n",
        "plt.legend()\n",
        "plt.savefig('outputs/loss.png')\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting train_dcgan.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "devjhfvH0bgh",
        "outputId": "0afc6c55-f912-42c3-8e77-22615848fb3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python train_dcgan.py"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "##### GENERATOR #####\n",
            "Generator(\n",
            "  (main): Sequential(\n",
            "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (13): Tanh()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "\n",
            "##### DISCRIMINATOR #####\n",
            "Discriminator(\n",
            "  (main): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (12): Sigmoid()\n",
            "  )\n",
            ")\n",
            "######################\n",
            "  0% 0/390 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([128, 1])) that is different to the input size (torch.Size([128, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "100% 390/390 [00:42<00:00,  9.17it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([80, 1])) that is different to the input size (torch.Size([80])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([80, 1])) that is different to the input size (torch.Size([80, 1, 1, 1])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n",
            "391it [00:42,  9.26it/s]             \n",
            "Epoch 1 of 25\n",
            "Generator loss: 7.21858788, Discriminator loss: 0.61281002\n",
            "391it [00:42,  9.25it/s]             \n",
            "Epoch 2 of 25\n",
            "Generator loss: 4.35440826, Discriminator loss: 0.66974616\n",
            "391it [00:42,  9.26it/s]             \n",
            "Epoch 3 of 25\n",
            "Generator loss: 3.88588595, Discriminator loss: 0.58390570\n",
            "391it [00:42,  9.22it/s]             \n",
            "Epoch 4 of 25\n",
            "Generator loss: 3.34555602, Discriminator loss: 0.70985907\n",
            "391it [00:42,  9.17it/s]             \n",
            "Epoch 5 of 25\n",
            "Generator loss: 3.15687275, Discriminator loss: 0.70247555\n",
            "391it [00:42,  9.14it/s]             \n",
            "Epoch 6 of 25\n",
            "Generator loss: 2.89293289, Discriminator loss: 0.69860512\n",
            "391it [00:42,  9.28it/s]             \n",
            "Epoch 7 of 25\n",
            "Generator loss: 2.87367797, Discriminator loss: 0.74223816\n",
            "391it [00:42,  9.29it/s]             \n",
            "Epoch 8 of 25\n",
            "Generator loss: 2.88054228, Discriminator loss: 0.76568502\n",
            "391it [00:42,  9.31it/s]             \n",
            "Epoch 9 of 25\n",
            "Generator loss: 3.01380754, Discriminator loss: 0.72341317\n",
            "391it [00:42,  9.25it/s]             \n",
            "Epoch 10 of 25\n",
            "Generator loss: 3.33921695, Discriminator loss: 0.56407464\n",
            "391it [00:42,  9.29it/s]             \n",
            "Epoch 11 of 25\n",
            "Generator loss: 2.99463964, Discriminator loss: 0.77265418\n",
            "391it [00:42,  9.27it/s]             \n",
            "Epoch 12 of 25\n",
            "Generator loss: 3.21476340, Discriminator loss: 0.63277823\n",
            "391it [00:42,  9.30it/s]             \n",
            "Epoch 13 of 25\n",
            "Generator loss: 3.18651938, Discriminator loss: 0.62365335\n",
            "391it [00:42,  9.29it/s]             \n",
            "Epoch 14 of 25\n",
            "Generator loss: 3.28677678, Discriminator loss: 0.60548401\n",
            "391it [00:42,  9.30it/s]             \n",
            "Epoch 15 of 25\n",
            "Generator loss: 3.17121053, Discriminator loss: 0.64567566\n",
            "391it [00:42,  9.30it/s]             \n",
            "Epoch 16 of 25\n",
            "Generator loss: 3.27850723, Discriminator loss: 0.54506367\n",
            "391it [00:41,  9.34it/s]             \n",
            "Epoch 17 of 25\n",
            "Generator loss: 3.53785419, Discriminator loss: 0.54283643\n",
            "391it [00:42,  9.27it/s]             \n",
            "Epoch 18 of 25\n",
            "Generator loss: 2.62252879, Discriminator loss: 0.78406256\n",
            "391it [00:42,  9.27it/s]             \n",
            "Epoch 19 of 25\n",
            "Generator loss: 3.73625541, Discriminator loss: 0.43869969\n",
            "391it [00:42,  9.27it/s]             \n",
            "Epoch 20 of 25\n",
            "Generator loss: 7.40066671, Discriminator loss: 0.13214545\n",
            "391it [00:42,  9.31it/s]             \n",
            "Epoch 21 of 25\n",
            "Generator loss: 3.50262713, Discriminator loss: 0.58666247\n",
            "391it [00:42,  9.29it/s]             \n",
            "Epoch 22 of 25\n",
            "Generator loss: 2.81894207, Discriminator loss: 0.76052582\n",
            "391it [00:42,  9.29it/s]             \n",
            "Epoch 23 of 25\n",
            "Generator loss: 2.48583984, Discriminator loss: 0.77566624\n",
            "391it [00:42,  9.27it/s]             \n",
            "Epoch 24 of 25\n",
            "Generator loss: 2.73468328, Discriminator loss: 0.74806690\n",
            "391it [00:42,  9.29it/s]             \n",
            "Epoch 25 of 25\n",
            "Generator loss: 2.76803184, Discriminator loss: 0.72096115\n",
            "DONE TRAINING\n",
            "Traceback (most recent call last):\n",
            "  File \"train_dcgan.py\", line 163, in <module>\n",
            "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
            "  File \"train_dcgan.py\", line 163, in <listcomp>\n",
            "    imgs = [np.array(to_pil_image(img)) for img in images]\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\", line 136, in __call__\n",
            "    return F.to_pil_image(pic, self.mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\", line 124, in to_pil_image\n",
            "    raise ValueError('pic should be 2/3 dimensional. Got {} dimensions.'.format(pic.ndimension()))\n",
            "ValueError: pic should be 2/3 dimensional. Got 4 dimensions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WATAVTpR8eWZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}